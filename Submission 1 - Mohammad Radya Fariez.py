# -*- coding: utf-8 -*-
"""MLOps Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U2ZzjxeKHBaxlSa-SgDOjkPLd_5PUGRz

**Install Tfx Package**
"""

!pip install tfx

"""**Clone Github Repositories**

Untuk mendapatkan dataset dan pakage requirements 
"""

!git clone https://github.com/radyafariez/Twitter-US-Airline-Sentiment

pip install jupyter scikit-learn tensorflow tfx==1.11.0 flask joblib

pip install -r /content/Twitter-US-Airline-Sentiment/requirements.txt

pip install pypiwin32

"""**Import Libraries**"""

import os
import pandas as pd
import tensorflow_model_analysis as tfma
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator
from tfx.components import Transform, Trainer, Tuner, Evaluator, Pusher
from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

"""**Set Variables**

Untuk menentukan pipeline directories untuk menyimpan artifact dan output untuk Serving Model
"""

PIPELINE_NAME = "usairline-pipeline" 
SCHEMA_PIPELINE_NAME = "usairline-tfdv-schema"

PIPELINE_ROOT = os.path.join('/content/pipeline', PIPELINE_NAME)
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

tweets_data = pd.read_csv("/content/Twitter-US-Airline-Sentiment/data/Tweets.csv")
tweets_data.head(3)

"""Menentukan path data dan drop kolom yang tidak diperlukan"""

data_path = "data"

df = tweets_data.drop(["name", "tweet_coord", "tweet_created", "tweet_location", "user_timezone", "retweet_count"], axis=1)

if not os.path.exists(data_path):
    os.makedirs(data_path)

df.to_csv(os.path.join(data_path, "tweet.csv"), index=False)
df.sample(3)

DATA_ROOT = "/content/data" 
interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""**Data Ingestion**

Load data menggunakan ExampleGen
"""

output = example_gen_pb2.Output(
    split_config=example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2),
    ])
)

example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)
interactive_context.run(example_gen)

"""Create Summary Statistic Dataset"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)
  
interactive_context.run(statistics_gen)

interactive_context.show(statistics_gen.outputs["statistics"])

"""Create Data Schema"""

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"]
)
interactive_context.run(schema_gen)

interactive_context.show(schema_gen.outputs["schema"])

"""Create Data Validation"""

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

interactive_context.show(example_validator.outputs['anomalies'])

"""**Data Preprocessing**"""

TRANSFORM_MODULE_FILE = "tweets_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# 
# import tensorflow as tf
# import string
# import tensorflow_transform as tft
# 
# LABEL_KEY = "airline_sentiment_confidence"
# FEATURE_KEY = "text"
# 
# def transformed_name(key):
#     return f"{key}_xf"
# 
# def preprocessing_fn(inputs):
#     outputs = dict()
#     
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(
#         inputs[FEATURE_KEY]
#     )
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
#     
#     return outputs

transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)
interactive_context.run(transform)

"""Mengubah nama fitur yang telah ditransform"""

def transformed_name(key):
    """Renaming transformed features"""
    return key + "_xf"

"""Load data dalam format TFRecord"""

def gzip_reader_fn(filenames):
    """Loads compressed data"""
    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')

"""Fungsi input_fn()"""

import keras_tuner as kt
import tensorflow as tf
import tensorflow_transform as tft
from typing import NamedTuple, Dict, Text, Any
from keras_tuner.engine import base_tuner
from tensorflow.keras import layers
from tfx.components.trainer.fn_args_utils import FnArgs

def input_fn(file_pattern, 
             tf_transform_output,
             num_epochs,
             batch_size=64)->tf.data.Dataset:
    """Get post_tranform feature & create batches of data"""
    
    # Get post_transform feature spec
    transform_feature_spec = (
        tf_transform_output.transformed_feature_spec().copy())
    
    # create batches of data
    dataset = tf.data.experimental.make_batched_features_dataset(
        file_pattern=file_pattern,
        batch_size=batch_size,
        features=transform_feature_spec,
        reader=gzip_reader_fn,
        num_epochs=num_epochs,
        label_key = transformed_name(LABEL_KEY))
    return dataset

"""Model bulder, membangun arsitektur model dengan menggunakan _embedding layer_"""

from tensorflow.keras import layers

VOCAB_SIZE = 10000
SEQUENCE_LENGTH = 100
 
vectorize_layer = layers.TextVectorization(
    standardize="lower_and_strip_punctuation",
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=SEQUENCE_LENGTH)
 
embedding_dim=16
def model_builder():
    """Build machine learning model"""
    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
    reshaped_narrative = tf.reshape(inputs, [-1])
    x = vectorize_layer(reshaped_narrative)
    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation="relu")(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs = outputs)
    
    model.compile(
        loss = 'binary_crossentropy',
        optimizer=tf.keras.optimizers.Adam(0.01),
        metrics=[tf.keras.metrics.BinaryAccuracy()]
    
    )
  
    model.summary()
    return model

"""Fungsi _get_serve_tf_examples_fn() untuk menjalankan data preprocessing"""

def _get_serve_tf_examples_fn(model, tf_transform_output):
    
    model.tft_layer = tf_transform_output.transform_features_layer()
    
    @tf.function
    def serve_tf_examples_fn(serialized_tf_examples):
        
        feature_spec = tf_transform_output.raw_feature_spec()
        
        feature_spec.pop(LABEL_KEY)
        
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
        
        transformed_features = model.tft_layer(parsed_features)
        
        # get predictions using the transformed features
        return model(transformed_features)
        
    return serve_tf_examples_fn

"""Run function untuk menjalankan training model berdasarkan parameter training"""

from tfx.components.trainer.fn_args_utils import FnArgs

def run_fn(fn_args: FnArgs) -> None:
    
    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
    
    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir = log_dir, update_freq='batch'
    )
    
    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
    
    
    # Load the transform output
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
    
    # Create batches of data
    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
    vectorize_layer.adapt(
        [j[0].numpy()[0] for j in [
            i[0][transformed_name(FEATURE_KEY)]
                for i in list(train_set)]])
    
    # Build the model
    model = model_builder()
    
    
    # Train the model
    model.fit(x = train_set,
            validation_data = val_set,
            callbacks = [tensorboard_callback, es, mc],
            steps_per_epoch = 1000, 
            validation_steps= 1000,
            epochs=10)
    signatures = {
        'serving_default':
        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
                                    tf.TensorSpec(
                                    shape=[None],
                                    dtype=tf.string,
                                    name='examples'))
    }
    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

"""**Training Model**

Mendefinisikan seluruh function yang disatukan ke module trainer
"""

TRAINER_MODULE_FILE = "tweets_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import tensorflow as tf
# import tensorflow_transform as tft 
# from tensorflow.keras import layers
# import os  
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
#  
# LABEL_KEY = "airline_sentiment_confidence"
# FEATURE_KEY = "text"
#  
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
#  
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
#  
#  
# def input_fn(file_pattern, 
#              tf_transform_output,
#              num_epochs,
#              batch_size=64)->tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
#     
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
#     
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = transformed_name(LABEL_KEY))
#     return dataset
#  
# # os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'
# # embed = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4")
#  
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE = 10000
# SEQUENCE_LENGTH = 100
#  
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH)
#  
#  
# embedding_dim=16
# def model_builder():
#     """Build machine learning model"""
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation='relu')(x)
#     x = layers.Dense(32, activation="relu")(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
#     
#     
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
#     
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
#     
#     )
#     
#     # print(model)
#     model.summary()
#     return model 
#  
#  
# def _get_serve_tf_examples_fn(model, tf_transform_output):
#     
#     model.tft_layer = tf_transform_output.transform_features_layer()
#     
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
#         
#         feature_spec = tf_transform_output.raw_feature_spec()
#         
#         feature_spec.pop(LABEL_KEY)
#         
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
#         
#         transformed_features = model.tft_layer(parsed_features)
#         
#         # get predictions using the transformed features
#         return model(transformed_features)
#         
#     return serve_tf_examples_fn
#     
# def run_fn(fn_args: FnArgs) -> None:
#     
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
#     
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = log_dir, update_freq='batch'
#     )
#     
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
#     
#     
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
#     
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(FEATURE_KEY)]
#                 for i in list(train_set)]])
#     
#     # Build the model
#     model = model_builder()
#     
#     
#     # Train the model
#     model.fit(x = train_set,
#             validation_data = val_set,
#             callbacks = [tensorboard_callback, es, mc],
#             steps_per_epoch = 1000, 
#             validation_steps= 1000,
#             epochs=10)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }
#     model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

"""Mendefinisikan komponen Trainer dengan input **module_file**, **examples**, **transform_graph**, **schema**, **train_args**, **eval_args**"""

from tfx.proto import trainer_pb2
 
trainer  = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)
interactive_context.run(trainer)

"""**Analisis dan Validasi Model**

Resolver component, membuat baseline model
"""

from tfx.dsl.components.common.resolver import Resolver 
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy 
from tfx.types import Channel 
from tfx.types.standard_artifacts import Model, ModelBlessing 
 
model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')
 
interactive_context.run(model_resolver)

"""Evaluasi model dengan komponen Evaluator menggunakan library TFMA"""

import tensorflow_model_analysis as tfma 
 
eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='airline_sentiment_confidence')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[
            
            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]
 
)

"""Mendefinisikan komponen Evaluator"""

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)
 
interactive_context.run(evaluator)

"""Visualisasi hasil"""

# Visualisasi hasil Evaluator
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""**Export Model**"""

pusher = Pusher(
model = trainer.outputs['model'],
model_blessing = evaluator.outputs['blessing'],
push_destination = pusher_pb2.PushDestination(
    filesystem = pusher_pb2.PushDestination.Filesystem(
        base_directory = os.path.join(
            SERVING_MODEL_DIR, "twitter-us-airline-sentiment"
        ),
    )
)
)

interactive_context.run(pusher)

"""**Compress Folders**"""

!zip -r /content/radyafariez-pipeline.zip /content/pipeline

!zip -r /content/serving_model_dir.zip /content/serving_model

!pip freeze >> requirements.txt